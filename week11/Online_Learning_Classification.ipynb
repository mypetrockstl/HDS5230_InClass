{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online_Learning_Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Author: Evan Carey*\n",
    "\n",
    "*Copyright 2017-2019, BH Analytics, LLC*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this lecture set, we will go over machine learning classification methods appropriate for online learning (chunked optimization). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case where our outcome (target) variable is discrete with a limited number of possible values, we can use classification algorithms to predict the outcome. Imagine a binary outcome with values of 'Yes' and 'No'. We are interested in predicting the probability that the outcome is either 'Yes' or 'No'. It is also possible to predict outcomes with more than two possible values, but we will focus on the binary case here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Modules\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set default figure size to be larger \n",
    "## this may only work in matplotlib 2.0+!\n",
    "matplotlib.rcParams['figure.figsize'] = [10.0,6.0]\n",
    "## Enable multiple outputs from jupyter cells\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.7 (default, Sep 16 2021, 13:09:58) \n",
      "[GCC 7.5.0]\n",
      "Pandas version: 1.3.4\n",
      "Matplotlib version: 3.4.3\n",
      "Numpy version: 1.20.3\n",
      "SciKitLearn version: 0.24.2\n",
      "Dask version: 2021.10.0\n"
     ]
    }
   ],
   "source": [
    "## Get Version information\n",
    "print(sys.version)\n",
    "print(\"Pandas version: {0}\".format(pd.__version__))\n",
    "print(\"Matplotlib version: {0}\".format(matplotlib.__version__))\n",
    "print(\"Numpy version: {0}\".format(np.__version__))\n",
    "print(\"SciKitLearn version: {0}\".format(sklearn.__version__))\n",
    "print(\"Dask version: {0}\".format(dask.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your working directory\n",
    "\n",
    "Set your working directory to make paths easier :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My working directory:\n",
      "/home/s/teaching/zarchives/Spring2021/HDS5230/notes/week12/Uploads-week-12/Uploads-week-12\n"
     ]
    }
   ],
   "source": [
    "# Working Directory\n",
    "import os\n",
    "print(\"My working directory:\\n\" + os.getcwd())\n",
    "# Set Working Directory \n",
    "#os.chdir(r\"C:\\Users\\evancarey\\Dropbox\\Work\")\n",
    "#print(\"My new working directory:\\n\" + os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two different data scenarios for this lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider two different possible data scenarios for this lecture set. \n",
    "\n",
    "1. The first scenario is that we are working on a large dataset that can fit into RAM. However...this dataset is big enough that we are having issues fitting models on the dataset when we try to converge models. (It **barely** fits into RAM)\n",
    "\n",
    "2. The second scenario is when the dataset is too big to fit into RAM, so we must try to work on it one piece at a time without ever having the full dataset in RAM!\n",
    "\n",
    "The techniques we use are different in these two situations. First, we consider data that fits into RAM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patient Mortality Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a dataset with a binary outcome of mortality as a motivating example.\n",
    "\n",
    "This is a dataset of patients demographics and disease status, with mortality indicated. The dataset is here: \n",
    "\n",
    "`data\\healthcare\\patientAnalyticFile.csv`\n",
    "\n",
    "In practice, you most likely would have created a dataset like this from multiple other files after cleaning, reshaping, and joining them. \n",
    "\n",
    "You can generalize this setup to any situation with a binary outcome, such as estimating the probability of a customer filing a warranty claim, or the probability of a transaction being fraudulent. \n",
    "\n",
    "We will first import this dataset and examine the potential variables to use in our classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>DateOfBirth</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Myocardial_infarction</th>\n",
       "      <th>Congestive_heart_failure</th>\n",
       "      <th>Peripheral_vascular_disease</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>Dementia</th>\n",
       "      <th>Pulmonary</th>\n",
       "      <th>...</th>\n",
       "      <th>Metastatic_solid_tumour</th>\n",
       "      <th>HIV</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Drugs</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>First_Appointment_Date</th>\n",
       "      <th>Last_Appointment_Date</th>\n",
       "      <th>DateOfDeath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1962-02-27</td>\n",
       "      <td>female</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-04-27</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1959-08-18</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2005-11-30</td>\n",
       "      <td>2008-11-02</td>\n",
       "      <td>2008-11-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1946-02-15</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-11-05</td>\n",
       "      <td>2015-11-13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1979-07-27</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-03-01</td>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>2016-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1983-02-19</td>\n",
       "      <td>female</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2006-09-22</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>19996</td>\n",
       "      <td>1997-12-19</td>\n",
       "      <td>female</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-14</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>19997</td>\n",
       "      <td>1984-03-31</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007-04-24</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>19998</td>\n",
       "      <td>1993-07-04</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-16</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>19999</td>\n",
       "      <td>1984-04-17</td>\n",
       "      <td>male</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>20000</td>\n",
       "      <td>1966-05-14</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>2012-05-16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PatientID DateOfBirth  Gender      Race  Myocardial_infarction  \\\n",
       "0              1  1962-02-27  female  hispanic                      0   \n",
       "1              2  1959-08-18    male     white                      0   \n",
       "2              3  1946-02-15  female     white                      0   \n",
       "3              4  1979-07-27  female     white                      0   \n",
       "4              5  1983-02-19  female  hispanic                      0   \n",
       "...          ...         ...     ...       ...                    ...   \n",
       "19995      19996  1997-12-19  female     other                      0   \n",
       "19996      19997  1984-03-31  female     white                      0   \n",
       "19997      19998  1993-07-04  female     white                      0   \n",
       "19998      19999  1984-04-17    male     other                      0   \n",
       "19999      20000  1966-05-14  female     white                      0   \n",
       "\n",
       "       Congestive_heart_failure  Peripheral_vascular_disease  Stroke  \\\n",
       "0                             0                            0       0   \n",
       "1                             0                            0       0   \n",
       "2                             0                            0       0   \n",
       "3                             0                            0       0   \n",
       "4                             0                            0       0   \n",
       "...                         ...                          ...     ...   \n",
       "19995                         0                            0       0   \n",
       "19996                         0                            0       0   \n",
       "19997                         0                            0       0   \n",
       "19998                         0                            0       0   \n",
       "19999                         0                            0       0   \n",
       "\n",
       "       Dementia  Pulmonary  ...  Metastatic_solid_tumour  HIV  Obesity  \\\n",
       "0             0          0  ...                        0    0        0   \n",
       "1             0          0  ...                        0    0        0   \n",
       "2             0          0  ...                        0    1        0   \n",
       "3             0          1  ...                        0    0        0   \n",
       "4             0          0  ...                        0    0        0   \n",
       "...         ...        ...  ...                      ...  ...      ...   \n",
       "19995         0          0  ...                        0    0        0   \n",
       "19996         0          0  ...                        0    1        0   \n",
       "19997         0          0  ...                        0    0        1   \n",
       "19998         0          0  ...                        0    0        0   \n",
       "19999         0          0  ...                        0    0        0   \n",
       "\n",
       "       Depression  Hypertension  Drugs  Alcohol  First_Appointment_Date  \\\n",
       "0               0             0      0        0              2013-04-27   \n",
       "1               0             1      0        0              2005-11-30   \n",
       "2               0             1      0        0              2011-11-05   \n",
       "3               0             0      0        0              2010-03-01   \n",
       "4               0             1      0        0              2006-09-22   \n",
       "...           ...           ...    ...      ...                     ...   \n",
       "19995           0             0      0        0              2008-06-14   \n",
       "19996           0             1      0        0              2007-04-24   \n",
       "19997           0             1      0        0              2010-10-16   \n",
       "19998           0             1      0        0              2015-01-04   \n",
       "19999           0             0      1        0              2011-04-01   \n",
       "\n",
       "       Last_Appointment_Date  DateOfDeath  \n",
       "0                 2018-06-01          NaN  \n",
       "1                 2008-11-02   2008-11-02  \n",
       "2                 2015-11-13          NaN  \n",
       "3                 2016-01-17   2016-01-17  \n",
       "4                 2018-06-01          NaN  \n",
       "...                      ...          ...  \n",
       "19995             2018-06-01          NaN  \n",
       "19996             2018-06-01          NaN  \n",
       "19997             2018-06-01          NaN  \n",
       "19998             2018-06-01          NaN  \n",
       "19999             2012-05-16          NaN  \n",
       "\n",
       "[20000 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Set print limits\n",
    "pd.options.display.max_rows = 10\n",
    "## Import Data\n",
    "df_patient = \\\n",
    " pd.read_csv('PatientAnalyticFile.csv')\n",
    "df_patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make a variable to indicate mortality. We can do that based on the absence of 'date of death':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        1\n",
       "2        0\n",
       "3        1\n",
       "4        0\n",
       "        ..\n",
       "19995    0\n",
       "19996    0\n",
       "19997    0\n",
       "19998    0\n",
       "19999    0\n",
       "Name: mortality, Length: 20000, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create mortality variable\n",
    "df_patient['mortality'] = \\\n",
    "    np.where(df_patient['DateOfDeath'].isnull(),\n",
    "             0,1)\n",
    "# Examine\n",
    "df_patient['mortality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20000.000000\n",
       "mean         0.354700\n",
       "std          0.478434\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          1.000000\n",
       "max          1.000000\n",
       "Name: mortality, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patient['mortality'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>Myocardial_infarction</th>\n",
       "      <th>Congestive_heart_failure</th>\n",
       "      <th>Peripheral_vascular_disease</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>Dementia</th>\n",
       "      <th>Pulmonary</th>\n",
       "      <th>Rheumatic</th>\n",
       "      <th>Peptic_ulcer_disease</th>\n",
       "      <th>LiverMild</th>\n",
       "      <th>...</th>\n",
       "      <th>Cancer</th>\n",
       "      <th>LiverSevere</th>\n",
       "      <th>Metastatic_solid_tumour</th>\n",
       "      <th>HIV</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Drugs</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>mortality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10000.500000</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.043450</td>\n",
       "      <td>0.023950</td>\n",
       "      <td>0.028650</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>0.072650</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050450</td>\n",
       "      <td>0.051450</td>\n",
       "      <td>0.033150</td>\n",
       "      <td>0.006450</td>\n",
       "      <td>0.163450</td>\n",
       "      <td>0.106300</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.040050</td>\n",
       "      <td>0.079750</td>\n",
       "      <td>0.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5773.647028</td>\n",
       "      <td>0.208621</td>\n",
       "      <td>0.203873</td>\n",
       "      <td>0.152897</td>\n",
       "      <td>0.166825</td>\n",
       "      <td>0.174401</td>\n",
       "      <td>0.259568</td>\n",
       "      <td>0.110224</td>\n",
       "      <td>0.097762</td>\n",
       "      <td>0.095733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218877</td>\n",
       "      <td>0.220919</td>\n",
       "      <td>0.179033</td>\n",
       "      <td>0.080054</td>\n",
       "      <td>0.369785</td>\n",
       "      <td>0.308229</td>\n",
       "      <td>0.459524</td>\n",
       "      <td>0.196081</td>\n",
       "      <td>0.270913</td>\n",
       "      <td>0.478434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5000.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10000.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15000.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PatientID  Myocardial_infarction  Congestive_heart_failure  \\\n",
       "count  20000.000000           20000.000000              20000.000000   \n",
       "mean   10000.500000               0.045600                  0.043450   \n",
       "std     5773.647028               0.208621                  0.203873   \n",
       "min        1.000000               0.000000                  0.000000   \n",
       "25%     5000.750000               0.000000                  0.000000   \n",
       "50%    10000.500000               0.000000                  0.000000   \n",
       "75%    15000.250000               0.000000                  0.000000   \n",
       "max    20000.000000               1.000000                  1.000000   \n",
       "\n",
       "       Peripheral_vascular_disease        Stroke      Dementia     Pulmonary  \\\n",
       "count                 20000.000000  20000.000000  20000.000000  20000.000000   \n",
       "mean                      0.023950      0.028650      0.031400      0.072650   \n",
       "std                       0.152897      0.166825      0.174401      0.259568   \n",
       "min                       0.000000      0.000000      0.000000      0.000000   \n",
       "25%                       0.000000      0.000000      0.000000      0.000000   \n",
       "50%                       0.000000      0.000000      0.000000      0.000000   \n",
       "75%                       0.000000      0.000000      0.000000      0.000000   \n",
       "max                       1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "          Rheumatic  Peptic_ulcer_disease     LiverMild  ...        Cancer  \\\n",
       "count  20000.000000          20000.000000  20000.000000  ...  20000.000000   \n",
       "mean       0.012300              0.009650      0.009250  ...      0.050450   \n",
       "std        0.110224              0.097762      0.095733  ...      0.218877   \n",
       "min        0.000000              0.000000      0.000000  ...      0.000000   \n",
       "25%        0.000000              0.000000      0.000000  ...      0.000000   \n",
       "50%        0.000000              0.000000      0.000000  ...      0.000000   \n",
       "75%        0.000000              0.000000      0.000000  ...      0.000000   \n",
       "max        1.000000              1.000000      1.000000  ...      1.000000   \n",
       "\n",
       "        LiverSevere  Metastatic_solid_tumour           HIV       Obesity  \\\n",
       "count  20000.000000             20000.000000  20000.000000  20000.000000   \n",
       "mean       0.051450                 0.033150      0.006450      0.163450   \n",
       "std        0.220919                 0.179033      0.080054      0.369785   \n",
       "min        0.000000                 0.000000      0.000000      0.000000   \n",
       "25%        0.000000                 0.000000      0.000000      0.000000   \n",
       "50%        0.000000                 0.000000      0.000000      0.000000   \n",
       "75%        0.000000                 0.000000      0.000000      0.000000   \n",
       "max        1.000000                 1.000000      1.000000      1.000000   \n",
       "\n",
       "         Depression  Hypertension         Drugs       Alcohol     mortality  \n",
       "count  20000.000000  20000.000000  20000.000000  20000.000000  20000.000000  \n",
       "mean       0.106300      0.302900      0.040050      0.079750      0.354700  \n",
       "std        0.308229      0.459524      0.196081      0.270913      0.478434  \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "75%        0.000000      1.000000      0.000000      0.000000      1.000000  \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patient.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatientID                  int64\n",
       "DateOfBirth               object\n",
       "Gender                    object\n",
       "Race                      object\n",
       "Myocardial_infarction      int64\n",
       "                           ...  \n",
       "Alcohol                    int64\n",
       "First_Appointment_Date    object\n",
       "Last_Appointment_Date     object\n",
       "DateOfDeath               object\n",
       "mortality                  int64\n",
       "Length: 30, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patient.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should change date of birth to be an actual date and calculate age if we want to include it in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20000.000000\n",
       "mean        47.247474\n",
       "std         18.145086\n",
       "min         15.753593\n",
       "25%         31.733744\n",
       "50%         47.099247\n",
       "75%         62.924025\n",
       "max         78.743326\n",
       "Name: Age_years, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert dateofBirth to date\n",
    "df_patient['DateOfBirth'] = \\\n",
    "    pd.to_datetime(df_patient['DateOfBirth'])\n",
    "# Calculate age in years as of 2015-01-01\n",
    "df_patient['Age_years'] = \\\n",
    "    ((pd.to_datetime('2015-01-01') - df_patient['DateOfBirth']).dt.days/365.25)\n",
    "df_patient['Age_years'].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow into scikit-learn\n",
    "\n",
    "\n",
    "* There are a number of possible ways to prepare data for modeling in scikit-learn. \n",
    "* You must end up with a numeric ndarray of inputs (X) and a numeric ndarray matrix of the target (Y)\n",
    "* I prefer the following workflow:\n",
    "  * We use pandas to import and clean data\n",
    "  * We use Patsy to create the X and Y ndarrays\n",
    "  * Using categorical transformations (dummy coding) as needed\n",
    "  * Also can generate non-linear terms including splines\n",
    "  * Use scikit-learn for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Patsy to Create the Model Matrices\n",
    "\n",
    "We typically start out with a pandas dataframe for manipulation purposes, then we will use this dataframe as the input to the machine learning library. I created a pandas dataframe above to replicate this process. We will use the dmatrices function from the patsy library to easily generate the design matrices for the machine learning algorithms representing the inputs. THis handles the following:\n",
    "\n",
    "* drops rows with missing data\n",
    "* construct one-hot encoding for categorical variables\n",
    "* optionally adds constant intecercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PatientID', 'DateOfBirth', 'Gender', 'Race', 'Myocardial_infarction',\n",
       "       'Congestive_heart_failure', 'Peripheral_vascular_disease', 'Stroke',\n",
       "       'Dementia', 'Pulmonary', 'Rheumatic', 'Peptic_ulcer_disease',\n",
       "       'LiverMild', 'Diabetes_without_complications',\n",
       "       'Diabetes_with_complications', 'Paralysis', 'Renal', 'Cancer',\n",
       "       'LiverSevere', 'Metastatic_solid_tumour', 'HIV', 'Obesity',\n",
       "       'Depression', 'Hypertension', 'Drugs', 'Alcohol',\n",
       "       'First_Appointment_Date', 'Last_Appointment_Date', 'DateOfDeath',\n",
       "       'mortality', 'Age_years'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patient.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mortality ~ Drugs + Diabetes_without_complications + LiverSevere + Obesity + HIV + Hypertension + Peptic_ulcer_disease + Depression + Paralysis + Rheumatic + Myocardial_infarction + Age_years + Race + Pulmonary + Cancer + Diabetes_with_complications + Peripheral_vascular_disease + LiverMild + Stroke + Congestive_heart_failure + Renal + Gender + Dementia + Alcohol + Metastatic_solid_tumour'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create formula for all variables in model\n",
    "vars_remove = ['PatientID','First_Appointment_Date','DateOfBirth',\n",
    "               'Last_Appointment_Date','DateOfDeath','mortality']\n",
    "vars_left = set(df_patient.columns) - set(vars_remove)\n",
    "formula = \"mortality ~ \" + \" + \".join(vars_left)\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## only use subset of data so models fit in reasonable time\n",
    "df_patient_sub = \\\n",
    "    df_patient.sample(frac=0.1,\n",
    "                     random_state=32)    \n",
    "## use Patsy to create model matrices\n",
    "Y,X = dmatrices(formula,\n",
    "                df_patient_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DesignMatrix with shape (2000, 1)\n",
       "  mortality\n",
       "          0\n",
       "          0\n",
       "          1\n",
       "          1\n",
       "          0\n",
       "          0\n",
       "          1\n",
       "          1\n",
       "          0\n",
       "          0\n",
       "          1\n",
       "          0\n",
       "          1\n",
       "          0\n",
       "          1\n",
       "          0\n",
       "          1\n",
       "          0\n",
       "          0\n",
       "          1\n",
       "          0\n",
       "          1\n",
       "          0\n",
       "          0\n",
       "          0\n",
       "          0\n",
       "          1\n",
       "          1\n",
       "          0\n",
       "          0\n",
       "  [1970 rows omitted]\n",
       "  Terms:\n",
       "    'mortality' (column 0)\n",
       "  (to view full data, use np.asarray(this_obj))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DesignMatrix with shape (2000, 28)\n",
       "  Columns:\n",
       "    ['Intercept',\n",
       "     'Race[T.hispanic]',\n",
       "     'Race[T.other]',\n",
       "     'Race[T.white]',\n",
       "     'Gender[T.male]',\n",
       "     'Drugs',\n",
       "     'Diabetes_without_complications',\n",
       "     'LiverSevere',\n",
       "     'Obesity',\n",
       "     'HIV',\n",
       "     'Hypertension',\n",
       "     'Peptic_ulcer_disease',\n",
       "     'Depression',\n",
       "     'Paralysis',\n",
       "     'Rheumatic',\n",
       "     'Myocardial_infarction',\n",
       "     'Age_years',\n",
       "     'Pulmonary',\n",
       "     'Cancer',\n",
       "     'Diabetes_with_complications',\n",
       "     'Peripheral_vascular_disease',\n",
       "     'LiverMild',\n",
       "     'Stroke',\n",
       "     'Congestive_heart_failure',\n",
       "     'Renal',\n",
       "     'Dementia',\n",
       "     'Alcohol',\n",
       "     'Metastatic_solid_tumour']\n",
       "  Terms:\n",
       "    'Intercept' (column 0)\n",
       "    'Race' (columns 1:4)\n",
       "    'Gender' (column 4)\n",
       "    'Drugs' (column 5)\n",
       "    'Diabetes_without_complications' (column 6)\n",
       "    'LiverSevere' (column 7)\n",
       "    'Obesity' (column 8)\n",
       "    'HIV' (column 9)\n",
       "    'Hypertension' (column 10)\n",
       "    'Peptic_ulcer_disease' (column 11)\n",
       "    'Depression' (column 12)\n",
       "    'Paralysis' (column 13)\n",
       "    'Rheumatic' (column 14)\n",
       "    'Myocardial_infarction' (column 15)\n",
       "    'Age_years' (column 16)\n",
       "    'Pulmonary' (column 17)\n",
       "    'Cancer' (column 18)\n",
       "    'Diabetes_with_complications' (column 19)\n",
       "    'Peripheral_vascular_disease' (column 20)\n",
       "    'LiverMild' (column 21)\n",
       "    'Stroke' (column 22)\n",
       "    'Congestive_heart_failure' (column 23)\n",
       "    'Renal' (column 24)\n",
       "    'Dementia' (column 25)\n",
       "    'Alcohol' (column 26)\n",
       "    'Metastatic_solid_tumour' (column 27)\n",
       "  (to view full data, use np.asarray(this_obj))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Testing and Training Samples\n",
    "\n",
    "* The first step is to set aside a test sample of data that will allow us to estimate the generalization error post-fit. This protects against overfitting. \n",
    "* We can use “tuple unpacking” to assign the values (very pythonic :)\n",
    "* We can assign a random seed (state) and fraction to split.\n",
    "\n",
    " For simple random splits, scikit-learn has a function `train_test_split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split Data into training and sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X,\n",
    "                     np.ravel(Y), # prevents dimensionality error later!\n",
    "                     test_size=0.25,\n",
    "                     random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm the Output Dimensions\n",
    "\n",
    "* We can confirm the dimensions of the data are the same within test and train\n",
    "* The proportion should also be close to the test_size argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Confirm dimensions\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 28)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with the Null Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider a null model of simply predicting the most frequent class as a base model. Without any other information, I may predict based simply on the distribution of the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.64666667)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Null information rate\n",
    "1 - y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikitlearn has a built in dummy classifier that works similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(random_state=0, strategy='most_frequent')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.6466666666666666"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Dummy classifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='most_frequent',\n",
    "                      random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to write a small function that will print the scores from a dict so we can compare the models. I will store the model scores in the dict as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dict to store all these results:\n",
    "result_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Function to Print Results\n",
    "def get_results(x1):\n",
    "    print(\"\\n{0:20}   {1:4}    {2:4}\".format('Model','Train','Test'))\n",
    "    print('-------------------------------------------')\n",
    "    for i in x1.keys():\n",
    "        print(\"{0:20}   {1:<6.4}   {2:<6.4}\".format(i,x1[i][0],x1[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Score the Model on Training and Testing Set\n",
    "result_scores['Null'] = \\\n",
    "            (sklearn.metrics.accuracy_score(y_train,clf.predict(X_train)),\n",
    "             sklearn.metrics.accuracy_score(y_test,clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Null                   0.6467   0.608 \n"
     ]
    }
   ],
   "source": [
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with the SAG Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will start with a logistic regression model, using the `solver=sag` option. \n",
    "* SAG handles an L2 penalty (not L1)\n",
    "* For any SAG based approach, the model optimization is sensitive to the scale of the inputs. So we must scale our parameters on the way in! \n",
    "* We can use the pipeline approach to preprocess the data. \n",
    "\n",
    "Check the docs: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Also, if you want to pick something other than the default accuracy for your cross validation you can:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "Here is a good post on differences between classification metrics:\n",
    "\n",
    "https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scale', StandardScaler()),\n",
       "                ('logit',\n",
       "                 LogisticRegressionCV(Cs=20, cv=5, fit_intercept=False,\n",
       "                                      max_iter=500, random_state=10,\n",
       "                                      solver='sag'))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import linear model\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "## set our transformation\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "## Set our model\n",
    "clf = linear_model.LogisticRegressionCV(fit_intercept=False, # already have the intercept\n",
    "                                        solver='sag', # stochastic average gradient...\n",
    "                                        Cs=20,\n",
    "                                        cv=5,\n",
    "                                        penalty='l2',\n",
    "                                        max_iter=500, # may need to increase this from default for convergence! \n",
    "                                        random_state=10) \n",
    "## put together in pipeline\n",
    "pipe1 = Pipeline([(\"scale\", scaler),\n",
    "                  (\"logit\", clf)])\n",
    "## fit model using data with .fit\n",
    "pipe1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Score the Model on Training and Testing Set\n",
    "result_scores['logit_SAG'] = \\\n",
    "            (sklearn.metrics.accuracy_score(y_train,pipe1.predict(X_train)),\n",
    "             sklearn.metrics.accuracy_score(y_test,pipe1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Null                   0.6467   0.608 \n",
      "logit_SAG              0.6767   0.688 \n"
     ]
    }
   ],
   "source": [
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with the SAGA Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will start with a logistic regression model, using the `solver=saga` option. \n",
    "* SAGA handles an L1 penalty (not L2)\n",
    "* For any SAG based approach, the model optimization is sensitive to the scale of the inputs. So we must scale our parameters on the way in! \n",
    "* We can use the pipeline approach to preprocess the data. \n",
    "\n",
    "Check the docs: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Also, if you want to pick something other than the default accuracy for your cross validation you can:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "Here is a good post on differences between classification metrics:\n",
    "\n",
    "https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scale', StandardScaler()),\n",
       "                ('logit',\n",
       "                 LogisticRegressionCV(Cs=20, cv=5, fit_intercept=False,\n",
       "                                      max_iter=500, penalty='l1',\n",
       "                                      random_state=10, solver='saga'))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import linear model\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "## set our transformation\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "## Set our model\n",
    "clf = linear_model.LogisticRegressionCV(fit_intercept=False, # already have the intercept\n",
    "                                        solver='saga', # stochastic average gradient...\n",
    "                                        Cs=20,\n",
    "                                        cv=5,\n",
    "                                        penalty='l1',\n",
    "                                        max_iter=500, # may need to increase this from default for convergence! \n",
    "                                        random_state=10) \n",
    "## put together in pipeline\n",
    "pipe1 = Pipeline([(\"scale\", scaler),\n",
    "                  (\"logit\", clf)])\n",
    "## fit model using data with .fit\n",
    "pipe1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Score the Model on Training and Testing Set\n",
    "result_scores['logit_SAGA'] = \\\n",
    "            (sklearn.metrics.accuracy_score(y_train,pipe1.predict(X_train)),\n",
    "             sklearn.metrics.accuracy_score(y_test,pipe1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Null                   0.6467   0.608 \n",
      "logit_SAG              0.6767   0.688 \n",
      "logit_SAGA             0.682    0.688 \n"
     ]
    }
   ],
   "source": [
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent in Scikit-learn\n",
    "\n",
    "There is a routine in scikit-learn called SGD we can directly call, and specify the loss function and potential regularizer. This allows us to specify a logistic regression with L1, L2, or both regularizers. We specify logistic regression by saying  `loss='log'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 109.46, NNZs: 27, Bias: 0.000000, T: 1500, Avg. loss: 33.367018\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 63.37, NNZs: 27, Bias: 0.000000, T: 3000, Avg. loss: 18.324365\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 39.34, NNZs: 27, Bias: 0.000000, T: 4500, Avg. loss: 12.874834\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 26.38, NNZs: 27, Bias: 0.000000, T: 6000, Avg. loss: 9.091764\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 29.60, NNZs: 27, Bias: 0.000000, T: 7500, Avg. loss: 7.548377\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 22.33, NNZs: 27, Bias: 0.000000, T: 9000, Avg. loss: 6.438229\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 17.23, NNZs: 27, Bias: 0.000000, T: 10500, Avg. loss: 5.222644\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 17.31, NNZs: 27, Bias: 0.000000, T: 12000, Avg. loss: 5.001976\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 16.78, NNZs: 27, Bias: 0.000000, T: 13500, Avg. loss: 4.228054\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 14.30, NNZs: 27, Bias: 0.000000, T: 15000, Avg. loss: 3.693065\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 11.95, NNZs: 27, Bias: 0.000000, T: 16500, Avg. loss: 3.391882\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 10.28, NNZs: 27, Bias: 0.000000, T: 18000, Avg. loss: 3.185068\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 9.14, NNZs: 27, Bias: 0.000000, T: 19500, Avg. loss: 2.863938\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 11.11, NNZs: 27, Bias: 0.000000, T: 21000, Avg. loss: 2.863985\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 9.22, NNZs: 27, Bias: 0.000000, T: 22500, Avg. loss: 2.610832\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 8.69, NNZs: 27, Bias: 0.000000, T: 24000, Avg. loss: 2.350417\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 9.68, NNZs: 27, Bias: 0.000000, T: 25500, Avg. loss: 2.247042\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 9.59, NNZs: 27, Bias: 0.000000, T: 27000, Avg. loss: 2.215327\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 6.90, NNZs: 27, Bias: 0.000000, T: 28500, Avg. loss: 2.006453\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 8.30, NNZs: 27, Bias: 0.000000, T: 30000, Avg. loss: 1.950495\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 6.23, NNZs: 27, Bias: 0.000000, T: 31500, Avg. loss: 1.801290\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 4.52, NNZs: 27, Bias: 0.000000, T: 33000, Avg. loss: 1.809261\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 5.36, NNZs: 27, Bias: 0.000000, T: 34500, Avg. loss: 1.753174\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 6.34, NNZs: 27, Bias: 0.000000, T: 36000, Avg. loss: 1.813617\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 6.72, NNZs: 27, Bias: 0.000000, T: 37500, Avg. loss: 1.705099\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 5.50, NNZs: 27, Bias: 0.000000, T: 39000, Avg. loss: 1.533305\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 4.53, NNZs: 27, Bias: 0.000000, T: 40500, Avg. loss: 1.619509\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 5.06, NNZs: 27, Bias: 0.000000, T: 42000, Avg. loss: 1.484100\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 5.25, NNZs: 27, Bias: 0.000000, T: 43500, Avg. loss: 1.395862\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 5.47, NNZs: 27, Bias: 0.000000, T: 45000, Avg. loss: 1.415314\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 4.00, NNZs: 27, Bias: 0.000000, T: 46500, Avg. loss: 1.330783\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 4.53, NNZs: 27, Bias: 0.000000, T: 48000, Avg. loss: 1.274817\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 4.18, NNZs: 27, Bias: 0.000000, T: 49500, Avg. loss: 1.260316\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 3.81, NNZs: 27, Bias: 0.000000, T: 51000, Avg. loss: 1.245811\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 3.82, NNZs: 27, Bias: 0.000000, T: 52500, Avg. loss: 1.273402\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 3.41, NNZs: 27, Bias: 0.000000, T: 54000, Avg. loss: 1.271882\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 3.84, NNZs: 27, Bias: 0.000000, T: 55500, Avg. loss: 1.167395\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 3.29, NNZs: 27, Bias: 0.000000, T: 57000, Avg. loss: 1.221928\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 3.86, NNZs: 27, Bias: 0.000000, T: 58500, Avg. loss: 1.169616\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 3.88, NNZs: 27, Bias: 0.000000, T: 60000, Avg. loss: 1.173089\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 4.01, NNZs: 27, Bias: 0.000000, T: 61500, Avg. loss: 1.112509\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 3.70, NNZs: 27, Bias: 0.000000, T: 63000, Avg. loss: 1.089169\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 3.23, NNZs: 27, Bias: 0.000000, T: 64500, Avg. loss: 1.073411\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 3.41, NNZs: 27, Bias: 0.000000, T: 66000, Avg. loss: 1.100084\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 3.50, NNZs: 27, Bias: 0.000000, T: 67500, Avg. loss: 1.098339\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 3.09, NNZs: 27, Bias: 0.000000, T: 69000, Avg. loss: 0.994874\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 2.97, NNZs: 27, Bias: 0.000000, T: 70500, Avg. loss: 1.034550\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 3.31, NNZs: 27, Bias: 0.000000, T: 72000, Avg. loss: 0.991176\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 2.48, NNZs: 27, Bias: 0.000000, T: 73500, Avg. loss: 0.957658\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 2.68, NNZs: 27, Bias: 0.000000, T: 75000, Avg. loss: 1.006488\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 3.62, NNZs: 27, Bias: 0.000000, T: 76500, Avg. loss: 0.965163\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 2.79, NNZs: 27, Bias: 0.000000, T: 78000, Avg. loss: 0.962555\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 2.56, NNZs: 27, Bias: 0.000000, T: 79500, Avg. loss: 0.937150\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 2.88, NNZs: 27, Bias: 0.000000, T: 81000, Avg. loss: 0.973005\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 2.32, NNZs: 27, Bias: 0.000000, T: 82500, Avg. loss: 0.928068\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 2.68, NNZs: 27, Bias: 0.000000, T: 84000, Avg. loss: 0.937055\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 2.92, NNZs: 27, Bias: 0.000000, T: 85500, Avg. loss: 0.973239\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 3.14, NNZs: 27, Bias: 0.000000, T: 87000, Avg. loss: 0.911601\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 2.74, NNZs: 27, Bias: 0.000000, T: 88500, Avg. loss: 0.867471\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 2.37, NNZs: 27, Bias: 0.000000, T: 90000, Avg. loss: 0.897234\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 2.80, NNZs: 27, Bias: 0.000000, T: 91500, Avg. loss: 0.870896\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 3.15, NNZs: 27, Bias: 0.000000, T: 93000, Avg. loss: 0.864581\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 2.65, NNZs: 27, Bias: 0.000000, T: 94500, Avg. loss: 0.897879\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 3.05, NNZs: 27, Bias: 0.000000, T: 96000, Avg. loss: 0.862581\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 2.02, NNZs: 27, Bias: 0.000000, T: 97500, Avg. loss: 0.864327\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 2.08, NNZs: 27, Bias: 0.000000, T: 99000, Avg. loss: 0.874057\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 2.09, NNZs: 27, Bias: 0.000000, T: 100500, Avg. loss: 0.875384\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 2.60, NNZs: 27, Bias: 0.000000, T: 102000, Avg. loss: 0.846078\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 2.59, NNZs: 27, Bias: 0.000000, T: 103500, Avg. loss: 0.863600\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 2.16, NNZs: 27, Bias: 0.000000, T: 105000, Avg. loss: 0.853084\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 2.80, NNZs: 27, Bias: 0.000000, T: 106500, Avg. loss: 0.801515\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 2.72, NNZs: 27, Bias: 0.000000, T: 108000, Avg. loss: 0.816633\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 2.06, NNZs: 27, Bias: 0.000000, T: 109500, Avg. loss: 0.827310\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 2.47, NNZs: 27, Bias: 0.000000, T: 111000, Avg. loss: 0.841808\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 2.31, NNZs: 27, Bias: 0.000000, T: 112500, Avg. loss: 0.812567\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 2.11, NNZs: 27, Bias: 0.000000, T: 114000, Avg. loss: 0.830703\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 76 epochs took 0.02 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scale', StandardScaler()),\n",
       "                ('sgd',\n",
       "                 SGDClassifier(fit_intercept=False, loss='log', random_state=12,\n",
       "                               tol=1e-06, verbose=1))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "## set our transformation\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "## Set our model\n",
    "clf = linear_model.SGDClassifier(fit_intercept=False, # already have the intercept\n",
    "                                 loss='log',\n",
    "                                 shuffle=True,\n",
    "                                 verbose=1,\n",
    "                                 random_state=12,\n",
    "                                 max_iter=1000, # total number of potential epochs\n",
    "                                 tol=1e-6)\n",
    "## put together in pipeline\n",
    "pipe1 = Pipeline([(\"scale\", scaler),\n",
    "                  (\"sgd\", clf)])\n",
    "## fit model using data with .fit\n",
    "pipe1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Score the Model on Training and Testing Set\n",
    "result_scores['SAG'] = \\\n",
    "            (sklearn.metrics.accuracy_score(y_train,pipe1.predict(X_train)),\n",
    "             sklearn.metrics.accuracy_score(y_test,pipe1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Null                   0.6467   0.608 \n",
      "logit_SAG              0.6767   0.688 \n",
      "logit_SAGA             0.682    0.688 \n",
      "SAG                    0.6713   0.682 \n"
     ]
    }
   ],
   "source": [
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing regularization\n",
    "\n",
    "Perhaps we should consider regularizing this model? \n",
    "\n",
    "We can implement either elastic net, l1, or l2 penalties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.12, NNZs: 1, Bias: 0.000000, T: 1500, Avg. loss: 0.687885\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.11, NNZs: 1, Bias: 0.000000, T: 3000, Avg. loss: 0.684548\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.11, NNZs: 1, Bias: 0.000000, T: 4500, Avg. loss: 0.684819\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.11, NNZs: 1, Bias: 0.000000, T: 6000, Avg. loss: 0.684356\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.10, NNZs: 1, Bias: 0.000000, T: 7500, Avg. loss: 0.684601\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.10, NNZs: 1, Bias: 0.000000, T: 9000, Avg. loss: 0.684838\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.10, NNZs: 1, Bias: 0.000000, T: 10500, Avg. loss: 0.684678\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.10, NNZs: 1, Bias: 0.000000, T: 12000, Avg. loss: 0.684611\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.10, NNZs: 1, Bias: 0.000000, T: 13500, Avg. loss: 0.684671\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 9 epochs took 0.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scale', StandardScaler()),\n",
       "                ('sgd',\n",
       "                 SGDClassifier(alpha=1, fit_intercept=False, loss='log',\n",
       "                               penalty='elasticnet', random_state=12, tol=1e-06,\n",
       "                               verbose=1))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "## set our transformation\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "## Set our model\n",
    "clf = linear_model.SGDClassifier(fit_intercept=False, # already have the intercept\n",
    "                                 loss='log',\n",
    "                                 penalty='elasticnet',\n",
    "                                 alpha=1,\n",
    "                                 shuffle=True,\n",
    "                                 verbose=1,\n",
    "                                 random_state=12,\n",
    "                                 max_iter=1000, # total number of potential epochs\n",
    "                                 tol=1e-6)\n",
    "## put together in pipeline\n",
    "pipe1 = Pipeline([(\"scale\", scaler),\n",
    "                  (\"sgd\", clf)])\n",
    "## fit model using data with .fit\n",
    "pipe1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Score the Model on Training and Testing Set\n",
    "result_scores['SAG_en'] = \\\n",
    "            (sklearn.metrics.accuracy_score(y_train,pipe1.predict(X_train)),\n",
    "             sklearn.metrics.accuracy_score(y_test,pipe1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Null                   0.6467   0.608 \n",
      "logit_SAG              0.6767   0.688 \n",
      "logit_SAGA             0.682    0.688 \n",
      "SAG                    0.6713   0.682 \n",
      "SAG_en                 0.6607   0.684 \n"
     ]
    }
   ],
   "source": [
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should cross validate the penalty with SGD. We can do that using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scale', StandardScaler()),\n",
       "                ('sgd',\n",
       "                 GridSearchCV(cv=5,\n",
       "                              estimator=SGDClassifier(fit_intercept=False,\n",
       "                                                      loss='log',\n",
       "                                                      penalty='elasticnet',\n",
       "                                                      random_state=12,\n",
       "                                                      tol=1e-06),\n",
       "                              param_grid={'alpha': (0.001, 0.01, 0.5, 1, 2, 5,\n",
       "                                                    10),\n",
       "                                          'l1_ratio': (0, 0.1, 0.25, 0.5, 0.75,\n",
       "                                                       0.9, 1)},\n",
       "                              return_train_score=True))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "## specify grid\n",
    "parameters = {'alpha':(0.001,0.01,0.5,1,2,5,10),\n",
    "              'l1_ratio':(0,0.1,0.25,0.5,0.75,0.9,1)}\n",
    "## specify model without hyperparameters\n",
    "clf = linear_model.SGDClassifier(fit_intercept=False, # already have the intercept\n",
    "                                 loss='log',\n",
    "                                 penalty='elasticnet',\n",
    "                                 shuffle=True,\n",
    "                                 #verbose=1,\n",
    "                                 random_state=12,\n",
    "                                 max_iter=1000, # total number of potential epochs\n",
    "                                 tol=1e-6)\n",
    "## specify grid\n",
    "clf2 = GridSearchCV(clf,\n",
    "                    parameters,\n",
    "                    cv=5,\n",
    "                    return_train_score=True)\n",
    "## set our transformation\n",
    "scaler = preprocessing.StandardScaler()\n",
    "## put together in pipeline\n",
    "pipe1 = Pipeline([(\"scale\", scaler),\n",
    "                  (\"sgd\", clf2)])\n",
    "## fit model using data with .fit\n",
    "pipe1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01470766, 0.00952396, 0.00788884, 0.00776582, 0.00791755,\n",
       "        0.00781679, 0.00778399, 0.0339704 , 0.02443452, 0.01988082,\n",
       "        0.01378212, 0.01230869, 0.01019073, 0.00759149, 0.00481939,\n",
       "        0.00234766, 0.002384  , 0.00218372, 0.00222616, 0.00234599,\n",
       "        0.00224209, 0.00479159, 0.00234652, 0.00214643, 0.00220008,\n",
       "        0.00223441, 0.002108  , 0.00213828, 0.00377336, 0.00213628,\n",
       "        0.00222478, 0.00213389, 0.00210719, 0.00199995, 0.00201235,\n",
       "        0.00363708, 0.00220704, 0.00218554, 0.00206981, 0.00198359,\n",
       "        0.00208111, 0.00204444, 0.00390267, 0.0021224 , 0.00201788,\n",
       "        0.00199747, 0.00203338, 0.00198479, 0.00199366]),\n",
       " 'std_fit_time': array([4.91245134e-03, 2.41388360e-03, 8.89243431e-04, 8.69908658e-04,\n",
       "        5.03181439e-04, 9.39500468e-04, 7.75067663e-04, 6.55350001e-03,\n",
       "        8.09277385e-03, 3.72866921e-03, 3.33728366e-03, 3.80132909e-03,\n",
       "        3.80830236e-03, 1.52047910e-03, 1.01244779e-03, 1.51105655e-04,\n",
       "        2.07989208e-04, 1.22935370e-04, 1.01844693e-04, 1.38413959e-04,\n",
       "        1.83001193e-04, 1.23364306e-03, 1.01682659e-04, 1.64847950e-04,\n",
       "        1.25976587e-04, 5.66833641e-05, 1.13477085e-04, 1.48426057e-04,\n",
       "        2.87467109e-04, 1.48582290e-04, 9.61436157e-05, 9.33555827e-05,\n",
       "        1.00643641e-04, 5.23715268e-05, 6.75209835e-05, 3.41061490e-04,\n",
       "        6.62335223e-05, 1.43259405e-04, 1.13440211e-04, 1.32963201e-05,\n",
       "        1.76563303e-04, 1.37967704e-04, 3.09381892e-04, 7.54703835e-05,\n",
       "        6.41308929e-05, 5.71745450e-05, 5.68242262e-05, 3.05154919e-05,\n",
       "        3.48022533e-05]),\n",
       " 'mean_score_time': array([0.0003088 , 0.00029221, 0.00030751, 0.00031796, 0.00028853,\n",
       "        0.00028601, 0.00028696, 0.00031409, 0.00030184, 0.00029163,\n",
       "        0.00028663, 0.00029426, 0.00030665, 0.00028214, 0.0002902 ,\n",
       "        0.00028071, 0.00028176, 0.0002789 , 0.00029669, 0.00029182,\n",
       "        0.00028019, 0.00029759, 0.00029879, 0.00028477, 0.00028067,\n",
       "        0.00028453, 0.00028343, 0.00027685, 0.00027928, 0.00027881,\n",
       "        0.0002789 , 0.00027518, 0.00027866, 0.00027995, 0.00027437,\n",
       "        0.00027657, 0.00027604, 0.00027676, 0.00027575, 0.00028391,\n",
       "        0.00027823, 0.00027399, 0.00027962, 0.00027542, 0.0002759 ,\n",
       "        0.0002769 , 0.000279  , 0.00027871, 0.00027657]),\n",
       " 'std_score_time': array([3.40274016e-05, 8.34751080e-06, 4.68586592e-05, 5.68518288e-05,\n",
       "        1.36878015e-06, 3.92225737e-06, 8.05925332e-06, 3.79061848e-05,\n",
       "        1.57717089e-05, 7.31350692e-06, 3.70708308e-06, 8.77095259e-06,\n",
       "        4.35139446e-05, 5.66333212e-06, 1.72054318e-05, 8.34124359e-07,\n",
       "        2.66517422e-06, 2.18722247e-06, 2.79088679e-05, 1.07980193e-05,\n",
       "        2.53397565e-06, 2.00801104e-05, 3.36916413e-05, 1.37603671e-05,\n",
       "        4.83325681e-06, 1.70382398e-05, 1.34662393e-05, 2.10781442e-06,\n",
       "        1.45884118e-06, 4.58011957e-06, 8.14735974e-06, 2.50237308e-06,\n",
       "        6.99408125e-06, 1.09655959e-05, 2.05317022e-06, 2.14311648e-06,\n",
       "        1.40160927e-06, 2.86658065e-06, 4.11739982e-06, 1.88017805e-05,\n",
       "        5.14454039e-06, 3.25299414e-06, 2.28384590e-06, 2.56518891e-06,\n",
       "        1.91567511e-06, 7.75883073e-06, 1.21011463e-05, 9.92921073e-06,\n",
       "        3.25159590e-06]),\n",
       " 'param_alpha': masked_array(data=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "                    2, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 10, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_l1_ratio': masked_array(data=[0, 0.1, 0.25, 0.5, 0.75, 0.9, 1, 0, 0.1, 0.25, 0.5,\n",
       "                    0.75, 0.9, 1, 0, 0.1, 0.25, 0.5, 0.75, 0.9, 1, 0, 0.1,\n",
       "                    0.25, 0.5, 0.75, 0.9, 1, 0, 0.1, 0.25, 0.5, 0.75, 0.9,\n",
       "                    1, 0, 0.1, 0.25, 0.5, 0.75, 0.9, 1, 0, 0.1, 0.25, 0.5,\n",
       "                    0.75, 0.9, 1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'alpha': 0.001, 'l1_ratio': 0},\n",
       "  {'alpha': 0.001, 'l1_ratio': 0.1},\n",
       "  {'alpha': 0.001, 'l1_ratio': 0.25},\n",
       "  {'alpha': 0.001, 'l1_ratio': 0.5},\n",
       "  {'alpha': 0.001, 'l1_ratio': 0.75},\n",
       "  {'alpha': 0.001, 'l1_ratio': 0.9},\n",
       "  {'alpha': 0.001, 'l1_ratio': 1},\n",
       "  {'alpha': 0.01, 'l1_ratio': 0},\n",
       "  {'alpha': 0.01, 'l1_ratio': 0.1},\n",
       "  {'alpha': 0.01, 'l1_ratio': 0.25},\n",
       "  {'alpha': 0.01, 'l1_ratio': 0.5},\n",
       "  {'alpha': 0.01, 'l1_ratio': 0.75},\n",
       "  {'alpha': 0.01, 'l1_ratio': 0.9},\n",
       "  {'alpha': 0.01, 'l1_ratio': 1},\n",
       "  {'alpha': 0.5, 'l1_ratio': 0},\n",
       "  {'alpha': 0.5, 'l1_ratio': 0.1},\n",
       "  {'alpha': 0.5, 'l1_ratio': 0.25},\n",
       "  {'alpha': 0.5, 'l1_ratio': 0.5},\n",
       "  {'alpha': 0.5, 'l1_ratio': 0.75},\n",
       "  {'alpha': 0.5, 'l1_ratio': 0.9},\n",
       "  {'alpha': 0.5, 'l1_ratio': 1},\n",
       "  {'alpha': 1, 'l1_ratio': 0},\n",
       "  {'alpha': 1, 'l1_ratio': 0.1},\n",
       "  {'alpha': 1, 'l1_ratio': 0.25},\n",
       "  {'alpha': 1, 'l1_ratio': 0.5},\n",
       "  {'alpha': 1, 'l1_ratio': 0.75},\n",
       "  {'alpha': 1, 'l1_ratio': 0.9},\n",
       "  {'alpha': 1, 'l1_ratio': 1},\n",
       "  {'alpha': 2, 'l1_ratio': 0},\n",
       "  {'alpha': 2, 'l1_ratio': 0.1},\n",
       "  {'alpha': 2, 'l1_ratio': 0.25},\n",
       "  {'alpha': 2, 'l1_ratio': 0.5},\n",
       "  {'alpha': 2, 'l1_ratio': 0.75},\n",
       "  {'alpha': 2, 'l1_ratio': 0.9},\n",
       "  {'alpha': 2, 'l1_ratio': 1},\n",
       "  {'alpha': 5, 'l1_ratio': 0},\n",
       "  {'alpha': 5, 'l1_ratio': 0.1},\n",
       "  {'alpha': 5, 'l1_ratio': 0.25},\n",
       "  {'alpha': 5, 'l1_ratio': 0.5},\n",
       "  {'alpha': 5, 'l1_ratio': 0.75},\n",
       "  {'alpha': 5, 'l1_ratio': 0.9},\n",
       "  {'alpha': 5, 'l1_ratio': 1},\n",
       "  {'alpha': 10, 'l1_ratio': 0},\n",
       "  {'alpha': 10, 'l1_ratio': 0.1},\n",
       "  {'alpha': 10, 'l1_ratio': 0.25},\n",
       "  {'alpha': 10, 'l1_ratio': 0.5},\n",
       "  {'alpha': 10, 'l1_ratio': 0.75},\n",
       "  {'alpha': 10, 'l1_ratio': 0.9},\n",
       "  {'alpha': 10, 'l1_ratio': 1}],\n",
       " 'split0_test_score': array([0.67333333, 0.67333333, 0.64666667, 0.64333333, 0.64333333,\n",
       "        0.64333333, 0.65      , 0.66333333, 0.66666667, 0.67333333,\n",
       "        0.67333333, 0.67666667, 0.67333333, 0.67666667, 0.66666667,\n",
       "        0.66666667, 0.66666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.66666667, 0.66666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.66333333, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.66333333, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.66      , 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667]),\n",
       " 'split1_test_score': array([0.67666667, 0.66      , 0.66333333, 0.67      , 0.67      ,\n",
       "        0.67      , 0.67      , 0.66666667, 0.66333333, 0.67333333,\n",
       "        0.68666667, 0.69      , 0.68      , 0.68      , 0.67333333,\n",
       "        0.67333333, 0.67333333, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.67333333, 0.67333333, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.67666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.67666667, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.67666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667]),\n",
       " 'split2_test_score': array([0.68333333, 0.67333333, 0.67333333, 0.67333333, 0.67333333,\n",
       "        0.67333333, 0.67333333, 0.67333333, 0.67333333, 0.67333333,\n",
       "        0.67666667, 0.68666667, 0.68666667, 0.68666667, 0.67333333,\n",
       "        0.67      , 0.67      , 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.68      , 0.67      , 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.68      , 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.68      , 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.68      , 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667]),\n",
       " 'split3_test_score': array([0.67      , 0.66333333, 0.66666667, 0.66666667, 0.66666667,\n",
       "        0.66666667, 0.66666667, 0.67666667, 0.67666667, 0.67666667,\n",
       "        0.68      , 0.67333333, 0.67      , 0.65666667, 0.67666667,\n",
       "        0.64      , 0.64      , 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.68      , 0.64      , 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.68      , 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.68      , 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.68333333, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667]),\n",
       " 'split4_test_score': array([0.67333333, 0.66      , 0.66      , 0.66      , 0.65666667,\n",
       "        0.65666667, 0.65666667, 0.65666667, 0.65333333, 0.66      ,\n",
       "        0.66333333, 0.66666667, 0.67      , 0.66333333, 0.66333333,\n",
       "        0.65333333, 0.65333333, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.66666667, 0.65333333, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.66666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.67      , 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.67      , 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667]),\n",
       " 'mean_test_score': array([0.67533333, 0.666     , 0.662     , 0.66266667, 0.662     ,\n",
       "        0.662     , 0.66333333, 0.66733333, 0.66666667, 0.67133333,\n",
       "        0.676     , 0.67866667, 0.676     , 0.67266667, 0.67066667,\n",
       "        0.66066667, 0.66066667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.67333333, 0.66066667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.67333333, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.674     , 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.674     , 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667]),\n",
       " 'std_test_score': array([0.00452155, 0.0061101 , 0.00884433, 0.01062492, 0.010873  ,\n",
       "        0.010873  , 0.00869227, 0.00711805, 0.00816497, 0.00581187,\n",
       "        0.00771722, 0.0085894 , 0.00646357, 0.0110353 , 0.00489898,\n",
       "        0.01236482, 0.01236482, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00596285, 0.01236482, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00699206, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00646357, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00827312, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ]),\n",
       " 'rank_test_score': array([ 4, 14, 17, 16, 17, 17, 15, 12, 13, 10,  2,  1,  2,  9, 11, 20, 20,\n",
       "        23, 23, 23, 23,  7, 20, 23, 23, 23, 23, 23,  7, 23, 23, 23, 23, 23,\n",
       "        23,  5, 23, 23, 23, 23, 23, 23,  5, 23, 23, 23, 23, 23, 23],\n",
       "       dtype=int32),\n",
       " 'split0_train_score': array([0.67833333, 0.69      , 0.67916667, 0.67833333, 0.67916667,\n",
       "        0.68      , 0.68083333, 0.67916667, 0.6825    , 0.67916667,\n",
       "        0.68333333, 0.6875    , 0.6875    , 0.6875    , 0.67583333,\n",
       "        0.65916667, 0.65916667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.68083333, 0.65916667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.6825    , 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.68416667, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.68416667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667]),\n",
       " 'split1_train_score': array([0.685     , 0.69083333, 0.69083333, 0.69083333, 0.69      ,\n",
       "        0.69083333, 0.69083333, 0.6975    , 0.69416667, 0.6875    ,\n",
       "        0.6825    , 0.68166667, 0.68083333, 0.68      , 0.68833333,\n",
       "        0.6575    , 0.6575    , 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.69      , 0.6575    , 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.69      , 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.68666667, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.68583333, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667]),\n",
       " 'split2_train_score': array([0.68333333, 0.685     , 0.685     , 0.68083333, 0.68083333,\n",
       "        0.68      , 0.6775    , 0.68583333, 0.68416667, 0.685     ,\n",
       "        0.68166667, 0.67916667, 0.68      , 0.67916667, 0.69      ,\n",
       "        0.65833333, 0.65833333, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.68916667, 0.65833333, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.6875    , 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.68916667, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.68833333, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667]),\n",
       " 'split3_train_score': array([0.68583333, 0.68583333, 0.685     , 0.68666667, 0.68666667,\n",
       "        0.68583333, 0.6875    , 0.68333333, 0.68166667, 0.68166667,\n",
       "        0.68583333, 0.68416667, 0.68      , 0.68166667, 0.69      ,\n",
       "        0.66583333, 0.66583333, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.68916667, 0.66583333, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.68916667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.68916667, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.69      , 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667]),\n",
       " 'split4_train_score': array([0.665     , 0.67833333, 0.67833333, 0.6775    , 0.6775    ,\n",
       "        0.67833333, 0.67666667, 0.6825    , 0.68333333, 0.67833333,\n",
       "        0.6825    , 0.685     , 0.68583333, 0.6825    , 0.68      ,\n",
       "        0.6625    , 0.6625    , 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.67833333, 0.6625    , 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.67916667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.6775    , 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.6775    , 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667]),\n",
       " 'mean_train_score': array([0.6795    , 0.686     , 0.68366667, 0.68283333, 0.68283333,\n",
       "        0.683     , 0.68266667, 0.68566667, 0.68516667, 0.68233333,\n",
       "        0.68316667, 0.6835    , 0.68283333, 0.68216667, 0.68483333,\n",
       "        0.66066667, 0.66066667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.6855    , 0.66066667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.68566667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.68533333, 0.64666667, 0.64666667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.68516667, 0.64666667, 0.64666667,\n",
       "        0.64666667, 0.64666667, 0.64666667, 0.64666667]),\n",
       " 'std_train_score': array([0.00770281, 0.00445346, 0.00455217, 0.00512619, 0.00473169,\n",
       "        0.00467262, 0.00558768, 0.00628932, 0.00457651, 0.00347211,\n",
       "        0.00143372, 0.00285774, 0.00318852, 0.00291548, 0.00583095,\n",
       "        0.00309121, 0.00309121, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00490465, 0.00309121, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00416333, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00433333, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00432692, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check best model params\n",
    "pipe1.named_steps['sgd'].cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.01, 'l1_ratio': 0.75}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.named_steps['sgd'].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Score the Model on Training and Testing Set\n",
    "result_scores['SAG_en_cv'] = \\\n",
    "            (sklearn.metrics.accuracy_score(y_train,pipe1.predict(X_train)),\n",
    "             sklearn.metrics.accuracy_score(y_test,pipe1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Null                   0.6467   0.608 \n",
      "logit_SAG              0.6767   0.688 \n",
      "logit_SAGA             0.682    0.688 \n",
      "SAG                    0.6713   0.682 \n",
      "SAG_en                 0.6607   0.684 \n",
      "SAG_en_cv              0.6833   0.688 \n"
     ]
    }
   ],
   "source": [
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming data and updating partial fits\n",
    "\n",
    "This is a bit more complex scenario. What if we want to fit a model on a dataset that we cannot have entirely in memory all at once? \n",
    "\n",
    "The SGD classifier supports a partial fit method where we can stream numpy arrays for each 'epoch', rather than making multiple passes over the data for each epoch. However, we must take care in preparing the datasets. Remember, the data is typically scaled and dummy coded prior to being passed into scikit-learn. We need to ensure that this is done in a uniform fashion when we stream over the dataset. \n",
    "\n",
    "The things that will get us are:\n",
    "\n",
    "* Consistently scaling the data\n",
    "* Ensuring the correct number of classes are specified (even if they aren't all represented in this subset of data)\n",
    "* Consistently dummy coding the data. \n",
    "\n",
    "You must code your way around this one way or another. I will show a basic approach below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "     PatientID DateOfBirth  Gender      Race  Myocardial_infarction  \\\n",
      "0            1  1962-02-27  female  hispanic                      0   \n",
      "1            2  1959-08-18    male     white                      0   \n",
      "2            3  1946-02-15  female     white                      0   \n",
      "3            4  1979-07-27  female     white                      0   \n",
      "4            5  1983-02-19  female  hispanic                      0   \n",
      "..         ...         ...     ...       ...                    ...   \n",
      "995        996  1964-07-28  female     black                      0   \n",
      "996        997  1980-10-07    male     white                      0   \n",
      "997        998  1949-09-06    male     black                      0   \n",
      "998        999  1945-07-20    male     white                      0   \n",
      "999       1000  1987-04-14    male     white                      0   \n",
      "\n",
      "     Congestive_heart_failure  Peripheral_vascular_disease  Stroke  Dementia  \\\n",
      "0                           0                            0       0         0   \n",
      "1                           0                            0       0         0   \n",
      "2                           0                            0       0         0   \n",
      "3                           0                            0       0         0   \n",
      "4                           0                            0       0         0   \n",
      "..                        ...                          ...     ...       ...   \n",
      "995                         0                            0       0         0   \n",
      "996                         0                            0       0         0   \n",
      "997                         0                            0       0         0   \n",
      "998                         0                            0       0         0   \n",
      "999                         0                            0       0         0   \n",
      "\n",
      "     Pulmonary  ...  Metastatic_solid_tumour  HIV  Obesity  Depression  \\\n",
      "0            0  ...                        0    0        0           0   \n",
      "1            0  ...                        0    0        0           0   \n",
      "2            0  ...                        0    1        0           0   \n",
      "3            1  ...                        0    0        0           0   \n",
      "4            0  ...                        0    0        0           0   \n",
      "..         ...  ...                      ...  ...      ...         ...   \n",
      "995          0  ...                        0    0        0           0   \n",
      "996          0  ...                        0    0        0           0   \n",
      "997          0  ...                        0    0        0           0   \n",
      "998          0  ...                        0    0        0           0   \n",
      "999          0  ...                        0    0        0           0   \n",
      "\n",
      "     Hypertension  Drugs  Alcohol  First_Appointment_Date  \\\n",
      "0               0      0        0              2013-04-27   \n",
      "1               1      0        0              2005-11-30   \n",
      "2               1      0        0              2011-11-05   \n",
      "3               0      0        0              2010-03-01   \n",
      "4               1      0        0              2006-09-22   \n",
      "..            ...    ...      ...                     ...   \n",
      "995             0      0        0              2013-10-22   \n",
      "996             1      0        0              2008-09-14   \n",
      "997             1      0        0              2006-08-22   \n",
      "998             1      0        0              2005-06-10   \n",
      "999             1      0        0              2006-04-11   \n",
      "\n",
      "     Last_Appointment_Date  DateOfDeath  \n",
      "0               2018-06-01          NaN  \n",
      "1               2008-11-02   2008-11-02  \n",
      "2               2015-11-13          NaN  \n",
      "3               2016-01-17   2016-01-17  \n",
      "4               2018-06-01          NaN  \n",
      "..                     ...          ...  \n",
      "995             2014-09-03          NaN  \n",
      "996             2018-06-01          NaN  \n",
      "997             2008-07-13   2008-07-13  \n",
      "998             2012-03-02   2012-03-02  \n",
      "999             2008-05-24   2008-05-24  \n",
      "\n",
      "[1000 rows x 29 columns]\n",
      "1\n",
      "      PatientID DateOfBirth  Gender      Race  Myocardial_infarction  \\\n",
      "1000       1001  1965-05-14    male     white                      0   \n",
      "1001       1002  1946-10-13    male  hispanic                      0   \n",
      "1002       1003  1975-07-27  female  hispanic                      0   \n",
      "1003       1004  1962-11-14    male     white                      0   \n",
      "1004       1005  1968-03-08    male     white                      0   \n",
      "...         ...         ...     ...       ...                    ...   \n",
      "1995       1996  1958-04-15  female     white                      0   \n",
      "1996       1997  1991-07-03    male     black                      0   \n",
      "1997       1998  1982-05-29  female     white                      0   \n",
      "1998       1999  1993-03-19  female  hispanic                      0   \n",
      "1999       2000  1951-05-31    male     black                      0   \n",
      "\n",
      "      Congestive_heart_failure  Peripheral_vascular_disease  Stroke  Dementia  \\\n",
      "1000                         0                            0       0         0   \n",
      "1001                         0                            0       0         0   \n",
      "1002                         0                            0       0         0   \n",
      "1003                         0                            0       0         0   \n",
      "1004                         0                            0       0         1   \n",
      "...                        ...                          ...     ...       ...   \n",
      "1995                         0                            0       0         0   \n",
      "1996                         0                            0       0         0   \n",
      "1997                         0                            0       0         0   \n",
      "1998                         0                            0       0         0   \n",
      "1999                         0                            0       0         0   \n",
      "\n",
      "      Pulmonary  ...  Metastatic_solid_tumour  HIV  Obesity  Depression  \\\n",
      "1000          0  ...                        0    0        0           0   \n",
      "1001          0  ...                        0    0        1           0   \n",
      "1002          0  ...                        0    0        0           0   \n",
      "1003          0  ...                        0    0        0           0   \n",
      "1004          0  ...                        0    0        1           0   \n",
      "...         ...  ...                      ...  ...      ...         ...   \n",
      "1995          0  ...                        0    0        0           0   \n",
      "1996          0  ...                        0    0        0           0   \n",
      "1997          0  ...                        0    0        0           0   \n",
      "1998          0  ...                        0    0        0           0   \n",
      "1999          0  ...                        0    0        0           1   \n",
      "\n",
      "      Hypertension  Drugs  Alcohol  First_Appointment_Date  \\\n",
      "1000             1      0        0              2007-07-23   \n",
      "1001             0      0        0              2006-09-12   \n",
      "1002             1      0        0              2009-02-26   \n",
      "1003             1      0        0              2009-09-15   \n",
      "1004             1      0        0              2012-02-12   \n",
      "...            ...    ...      ...                     ...   \n",
      "1995             1      0        1              2015-04-24   \n",
      "1996             0      0        0              2014-05-08   \n",
      "1997             0      0        0              2014-03-22   \n",
      "1998             1      0        0              2012-05-20   \n",
      "1999             0      0        0              2006-07-15   \n",
      "\n",
      "      Last_Appointment_Date  DateOfDeath  \n",
      "1000             2018-06-01          NaN  \n",
      "1001             2008-12-30   2008-12-30  \n",
      "1002             2018-06-01          NaN  \n",
      "1003             2018-06-01          NaN  \n",
      "1004             2018-06-01          NaN  \n",
      "...                     ...          ...  \n",
      "1995             2018-06-01          NaN  \n",
      "1996             2018-06-01          NaN  \n",
      "1997             2018-06-01          NaN  \n",
      "1998             2018-06-01          NaN  \n",
      "1999             2007-10-17   2007-10-17  \n",
      "\n",
      "[1000 rows x 29 columns]\n",
      "2\n",
      "      PatientID DateOfBirth  Gender      Race  Myocardial_infarction  \\\n",
      "2000       2001  1941-06-19  female     white                      0   \n",
      "2001       2002  1950-04-01  female     white                      0   \n",
      "2002       2003  1955-01-17    male     white                      0   \n",
      "2003       2004  1966-09-08    male     black                      0   \n",
      "2004       2005  1975-11-28  female     black                      0   \n",
      "...         ...         ...     ...       ...                    ...   \n",
      "2995       2996  1984-01-01    male     black                      0   \n",
      "2996       2997  1949-09-16  female  hispanic                      0   \n",
      "2997       2998  1948-11-17  female     other                      0   \n",
      "2998       2999  1992-05-07  female  hispanic                      0   \n",
      "2999       3000  1961-11-01  female  hispanic                      0   \n",
      "\n",
      "      Congestive_heart_failure  Peripheral_vascular_disease  Stroke  Dementia  \\\n",
      "2000                         0                            0       0         0   \n",
      "2001                         0                            0       0         0   \n",
      "2002                         0                            0       0         0   \n",
      "2003                         0                            0       0         0   \n",
      "2004                         0                            0       0         0   \n",
      "...                        ...                          ...     ...       ...   \n",
      "2995                         0                            0       0         0   \n",
      "2996                         0                            0       0         0   \n",
      "2997                         0                            0       0         0   \n",
      "2998                         0                            0       0         0   \n",
      "2999                         0                            0       0         0   \n",
      "\n",
      "      Pulmonary  ...  Metastatic_solid_tumour  HIV  Obesity  Depression  \\\n",
      "2000          0  ...                        0    0        0           0   \n",
      "2001          0  ...                        0    0        0           0   \n",
      "2002          0  ...                        1    0        1           0   \n",
      "2003          0  ...                        0    0        0           0   \n",
      "2004          0  ...                        0    0        0           0   \n",
      "...         ...  ...                      ...  ...      ...         ...   \n",
      "2995          0  ...                        0    0        1           0   \n",
      "2996          0  ...                        0    0        0           0   \n",
      "2997          0  ...                        0    0        0           0   \n",
      "2998          0  ...                        0    0        1           0   \n",
      "2999          1  ...                        0    0        0           0   \n",
      "\n",
      "      Hypertension  Drugs  Alcohol  First_Appointment_Date  \\\n",
      "2000             0      0        0              2007-05-22   \n",
      "2001             0      0        1              2009-03-15   \n",
      "2002             0      1        0              2014-09-20   \n",
      "2003             0      0        1              2013-01-16   \n",
      "2004             0      0        1              2011-07-31   \n",
      "...            ...    ...      ...                     ...   \n",
      "2995             0      0        0              2014-07-06   \n",
      "2996             0      1        0              2007-02-03   \n",
      "2997             0      0        0              2006-11-27   \n",
      "2998             1      0        0              2008-10-28   \n",
      "2999             0      0        0              2006-08-29   \n",
      "\n",
      "      Last_Appointment_Date  DateOfDeath  \n",
      "2000             2007-12-06   2007-12-06  \n",
      "2001             2018-06-01          NaN  \n",
      "2002             2018-06-01          NaN  \n",
      "2003             2014-06-03   2014-06-03  \n",
      "2004             2018-06-01          NaN  \n",
      "...                     ...          ...  \n",
      "2995             2016-07-09          NaN  \n",
      "2996             2007-12-11   2007-12-11  \n",
      "2997             2007-12-15   2007-12-15  \n",
      "2998             2016-05-09          NaN  \n",
      "2999             2012-03-27          NaN  \n",
      "\n",
      "[1000 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "## establish generator to yield data\n",
    "df_patient_gen = \\\n",
    " pd.read_csv('PatientAnalyticFile.csv',\n",
    "             chunksize=1000)\n",
    "## test\n",
    "for n,chunk in enumerate(df_patient_gen):\n",
    "    print(n)\n",
    "    print(chunk)\n",
    "    if n>1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>DateOfBirth</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Myocardial_infarction</th>\n",
       "      <th>Congestive_heart_failure</th>\n",
       "      <th>Peripheral_vascular_disease</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>Dementia</th>\n",
       "      <th>Pulmonary</th>\n",
       "      <th>...</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Drugs</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>First_Appointment_Date</th>\n",
       "      <th>Last_Appointment_Date</th>\n",
       "      <th>DateOfDeath</th>\n",
       "      <th>mortality</th>\n",
       "      <th>Age_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1962-02-27</td>\n",
       "      <td>female</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-04-27</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>52.843258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1959-08-18</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2005-11-30</td>\n",
       "      <td>2008-11-02</td>\n",
       "      <td>2008-11-02</td>\n",
       "      <td>1</td>\n",
       "      <td>55.373032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1946-02-15</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-11-05</td>\n",
       "      <td>2015-11-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>68.876112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1979-07-27</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-03-01</td>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>1</td>\n",
       "      <td>35.433265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1983-02-19</td>\n",
       "      <td>female</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2006-09-22</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>31.865845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1987-11-16</td>\n",
       "      <td>male</td>\n",
       "      <td>black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2006-10-22</td>\n",
       "      <td>2011-01-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>27.126626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1958-01-11</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-20</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>56.971937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1952-10-31</td>\n",
       "      <td>female</td>\n",
       "      <td>black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-03-25</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>62.168378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1951-10-06</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-08-04</td>\n",
       "      <td>2010-05-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>63.238877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1954-10-16</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>2015-10-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>60.210815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PatientID DateOfBirth  Gender      Race  Myocardial_infarction  \\\n",
       "0          1  1962-02-27  female  hispanic                      0   \n",
       "1          2  1959-08-18    male     white                      0   \n",
       "2          3  1946-02-15  female     white                      0   \n",
       "3          4  1979-07-27  female     white                      0   \n",
       "4          5  1983-02-19  female  hispanic                      0   \n",
       "5          6  1987-11-16    male     black                      0   \n",
       "6          7  1958-01-11    male     white                      0   \n",
       "7          8  1952-10-31  female     black                      0   \n",
       "8          9  1951-10-06  female     white                      0   \n",
       "9         10  1954-10-16    male     white                      0   \n",
       "\n",
       "   Congestive_heart_failure  Peripheral_vascular_disease  Stroke  Dementia  \\\n",
       "0                         0                            0       0         0   \n",
       "1                         0                            0       0         0   \n",
       "2                         0                            0       0         0   \n",
       "3                         0                            0       0         0   \n",
       "4                         0                            0       0         0   \n",
       "5                         0                            0       0         0   \n",
       "6                         0                            0       0         0   \n",
       "7                         0                            0       0         0   \n",
       "8                         0                            0       0         0   \n",
       "9                         0                            0       0         0   \n",
       "\n",
       "   Pulmonary  ...  Obesity  Depression  Hypertension  Drugs  Alcohol  \\\n",
       "0          0  ...        0           0             0      0        0   \n",
       "1          0  ...        0           0             1      0        0   \n",
       "2          0  ...        0           0             1      0        0   \n",
       "3          1  ...        0           0             0      0        0   \n",
       "4          0  ...        0           0             1      0        0   \n",
       "5          0  ...        0           0             0      0        0   \n",
       "6          0  ...        0           0             0      0        0   \n",
       "7          0  ...        0           0             1      0        0   \n",
       "8          0  ...        0           1             0      0        0   \n",
       "9          0  ...        0           0             0      0        0   \n",
       "\n",
       "   First_Appointment_Date  Last_Appointment_Date  DateOfDeath  mortality  \\\n",
       "0              2013-04-27             2018-06-01          NaN          0   \n",
       "1              2005-11-30             2008-11-02   2008-11-02          1   \n",
       "2              2011-11-05             2015-11-13          NaN          0   \n",
       "3              2010-03-01             2016-01-17   2016-01-17          1   \n",
       "4              2006-09-22             2018-06-01          NaN          0   \n",
       "5              2006-10-22             2011-01-13          NaN          0   \n",
       "6              2015-01-20             2018-06-01          NaN          0   \n",
       "7              2013-03-25             2018-06-01          NaN          0   \n",
       "8              2008-08-04             2010-05-23          NaN          0   \n",
       "9              2014-07-01             2015-10-19          NaN          0   \n",
       "\n",
       "   Age_years  \n",
       "0  52.843258  \n",
       "1  55.373032  \n",
       "2  68.876112  \n",
       "3  35.433265  \n",
       "4  31.865845  \n",
       "5  27.126626  \n",
       "6  56.971937  \n",
       "7  62.168378  \n",
       "8  63.238877  \n",
       "9  60.210815  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = \\\n",
    "    pd.read_csv('PatientAnalyticFile.csv',\n",
    "                nrows=10)\n",
    "# Create mortality variable\n",
    "df1['mortality'] = \\\n",
    "    np.where(df1['DateOfDeath'].isnull(),\n",
    "             0,1)\n",
    "# Convert dateofBirth to date\n",
    "df1['DateOfBirth'] = \\\n",
    "    pd.to_datetime(df1['DateOfBirth'])\n",
    "# Calculate age in years as of 2015-01-01\n",
    "df1['Age_years'] = \\\n",
    "    ((pd.to_datetime('2015-01-01') - df1['DateOfBirth']).dt.days/365.25)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PatientID', 'DateOfBirth', 'Gender', 'Race', 'Myocardial_infarction',\n",
       "       'Congestive_heart_failure', 'Peripheral_vascular_disease', 'Stroke',\n",
       "       'Dementia', 'Pulmonary', 'Rheumatic', 'Peptic_ulcer_disease',\n",
       "       'LiverMild', 'Diabetes_without_complications',\n",
       "       'Diabetes_with_complications', 'Paralysis', 'Renal', 'Cancer',\n",
       "       'LiverSevere', 'Metastatic_solid_tumour', 'HIV', 'Obesity',\n",
       "       'Depression', 'Hypertension', 'Drugs', 'Alcohol',\n",
       "       'First_Appointment_Date', 'Last_Appointment_Date', 'DateOfDeath',\n",
       "       'mortality', 'Age_years'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## numeric cols\n",
    "num_cols = ['Age_years']\n",
    "## categorical cols\n",
    "cat_cols = ['Gender', 'Race', 'Myocardial_infarction',\n",
    "       'Congestive_heart_failure', 'Peripheral_vascular_disease', 'Stroke',\n",
    "       'Dementia', 'Pulmonary', 'Rheumatic', 'Peptic_ulcer_disease',\n",
    "       'LiverMild', 'Diabetes_without_complications',\n",
    "       'Diabetes_with_complications', 'Paralysis', 'Renal', 'Cancer',\n",
    "       'LiverSevere', 'Metastatic_solid_tumour', 'HIV', 'Obesity',\n",
    "       'Depression', 'Hypertension', 'Drugs', 'Alcohol','mortality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age_years    28344.510245\n",
       "dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Age_years    514.108145\n",
       "dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate sum of X and X**2 for numeric variables as we go\n",
    "def sum2(x):\n",
    "    return(np.sum(x**2))\n",
    "df1.loc[:,num_cols].agg(sum2,axis=0)\n",
    "df1.loc[:,num_cols].agg(np.sum,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gender': {'female', 'male'},\n",
       " 'Race': {'black', 'hispanic', 'white'},\n",
       " 'Myocardial_infarction': {0},\n",
       " 'Congestive_heart_failure': {0},\n",
       " 'Peripheral_vascular_disease': {0},\n",
       " 'Stroke': {0},\n",
       " 'Dementia': {0},\n",
       " 'Pulmonary': {0, 1},\n",
       " 'Rheumatic': {0},\n",
       " 'Peptic_ulcer_disease': {0, 1},\n",
       " 'LiverMild': {0},\n",
       " 'Diabetes_without_complications': {0, 1},\n",
       " 'Diabetes_with_complications': {0},\n",
       " 'Paralysis': {0},\n",
       " 'Renal': {0},\n",
       " 'Cancer': {0, 1},\n",
       " 'LiverSevere': {0, 1},\n",
       " 'Metastatic_solid_tumour': {0},\n",
       " 'HIV': {0, 1},\n",
       " 'Obesity': {0},\n",
       " 'Depression': {0, 1},\n",
       " 'Hypertension': {0, 1},\n",
       " 'Drugs': {0},\n",
       " 'Alcohol': {0},\n",
       " 'mortality': {0, 1}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'female', 'male'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'female', 'male'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate unique values of categorical variables as we go\n",
    "dict_unique = {}\n",
    "for col in cat_cols:\n",
    "    dict_unique[col] = set(df1.loc[:,col])\n",
    "dict_unique\n",
    "dict_unique['Gender']\n",
    "dict_unique['Gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## establish generator to yield data\n",
    "df_patient_gen2 = \\\n",
    " pd.read_csv(r'PatientAnalyticFile.csv',\n",
    "             chunksize=100)\n",
    "## test\n",
    "for n,chunk in enumerate(df_patient_gen2):\n",
    "    ## process data first\n",
    "    # Create mortality variable\n",
    "    chunk['mortality'] = \\\n",
    "        np.where(chunk['DateOfDeath'].isnull(),\n",
    "                 0,1)\n",
    "    # Convert dateofBirth to date\n",
    "    chunk['DateOfBirth'] = \\\n",
    "        pd.to_datetime(chunk['DateOfBirth'])\n",
    "    # Calculate age in years as of 2015-01-01\n",
    "    chunk['Age_years'] = \\\n",
    "        ((pd.to_datetime('2015-01-01') - chunk['DateOfBirth']).dt.days/365.25)\n",
    "    \n",
    "    if n==0: ## for first chunk, establish new vars\n",
    "        n_rows = chunk.shape[0]\n",
    "        running_sum = chunk.loc[:,num_cols].agg(np.sum,axis=0)\n",
    "        running_sum_2 = chunk.loc[:,num_cols].agg(sum2,axis=0)\n",
    "        ## calculate unique values of categorical variables as we go\n",
    "        dict_unique = {}\n",
    "        for col in cat_cols:\n",
    "            dict_unique[col] = set(chunk.loc[:,col])\n",
    "    if n>0: ## for subsequent chunks, update these variables\n",
    "        n_rows = chunk.shape[0] + n_rows\n",
    "        running_sum = running_sum + chunk.loc[:,num_cols].agg(np.sum,axis=0)\n",
    "        running_sum_2 = running_sum_2 + chunk.loc[:,num_cols].agg(sum2,axis=0)\n",
    "        ## calculate unique values of categorical variables as we go\n",
    "        for col in cat_cols:\n",
    "            dict_unique[col] = set(chunk.loc[:,col]).union(set(dict_unique[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age_years']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Age_years    944949.478439\n",
       "dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'Gender': {'female', 'male'},\n",
       " 'Race': {'black', 'hispanic', 'other', 'white'},\n",
       " 'Myocardial_infarction': {0, 1},\n",
       " 'Congestive_heart_failure': {0, 1},\n",
       " 'Peripheral_vascular_disease': {0, 1},\n",
       " 'Stroke': {0, 1},\n",
       " 'Dementia': {0, 1},\n",
       " 'Pulmonary': {0, 1},\n",
       " 'Rheumatic': {0, 1},\n",
       " 'Peptic_ulcer_disease': {0, 1},\n",
       " 'LiverMild': {0, 1},\n",
       " 'Diabetes_without_complications': {0, 1},\n",
       " 'Diabetes_with_complications': {0, 1},\n",
       " 'Paralysis': {0, 1},\n",
       " 'Renal': {0, 1},\n",
       " 'Cancer': {0, 1},\n",
       " 'LiverSevere': {0, 1},\n",
       " 'Metastatic_solid_tumour': {0, 1},\n",
       " 'HIV': {0, 1},\n",
       " 'Obesity': {0, 1},\n",
       " 'Depression': {0, 1},\n",
       " 'Hypertension': {0, 1},\n",
       " 'Drugs': {0, 1},\n",
       " 'Alcohol': {0, 1},\n",
       " 'mortality': {0, 1}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check out the values we calculated:\n",
    "num_cols\n",
    "n_rows\n",
    "running_sum\n",
    "## unique vars\n",
    "dict_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age_years    47.247474\n",
       "dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Age_years    18.144632\n",
       "dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate mean\n",
    "mean_all = running_sum / n_rows\n",
    "## calculate stdev\n",
    "stdev_all = np.sqrt((running_sum_2 / n_rows) - (mean_all * mean_all)) \n",
    "## final nums to use for standardization:\n",
    "mean_all\n",
    "stdev_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a function that will use these metrics to generate appropriate X and Y matrices to feed into the scitlearn partial fit algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>DateOfBirth</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Myocardial_infarction</th>\n",
       "      <th>Congestive_heart_failure</th>\n",
       "      <th>Peripheral_vascular_disease</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>Dementia</th>\n",
       "      <th>Pulmonary</th>\n",
       "      <th>...</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Drugs</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>First_Appointment_Date</th>\n",
       "      <th>Last_Appointment_Date</th>\n",
       "      <th>DateOfDeath</th>\n",
       "      <th>mortality</th>\n",
       "      <th>Age_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1962-02-27</td>\n",
       "      <td>female</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-04-27</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>52.843258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1959-08-18</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2005-11-30</td>\n",
       "      <td>2008-11-02</td>\n",
       "      <td>2008-11-02</td>\n",
       "      <td>1</td>\n",
       "      <td>55.373032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1946-02-15</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-11-05</td>\n",
       "      <td>2015-11-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>68.876112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1979-07-27</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-03-01</td>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>1</td>\n",
       "      <td>35.433265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1983-02-19</td>\n",
       "      <td>female</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2006-09-22</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>31.865845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1987-11-16</td>\n",
       "      <td>male</td>\n",
       "      <td>black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2006-10-22</td>\n",
       "      <td>2011-01-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>27.126626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1958-01-11</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-20</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>56.971937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1952-10-31</td>\n",
       "      <td>female</td>\n",
       "      <td>black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-03-25</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>62.168378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1951-10-06</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-08-04</td>\n",
       "      <td>2010-05-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>63.238877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1954-10-16</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>2015-10-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>60.210815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PatientID DateOfBirth  Gender      Race  Myocardial_infarction  \\\n",
       "0          1  1962-02-27  female  hispanic                      0   \n",
       "1          2  1959-08-18    male     white                      0   \n",
       "2          3  1946-02-15  female     white                      0   \n",
       "3          4  1979-07-27  female     white                      0   \n",
       "4          5  1983-02-19  female  hispanic                      0   \n",
       "5          6  1987-11-16    male     black                      0   \n",
       "6          7  1958-01-11    male     white                      0   \n",
       "7          8  1952-10-31  female     black                      0   \n",
       "8          9  1951-10-06  female     white                      0   \n",
       "9         10  1954-10-16    male     white                      0   \n",
       "\n",
       "   Congestive_heart_failure  Peripheral_vascular_disease  Stroke  Dementia  \\\n",
       "0                         0                            0       0         0   \n",
       "1                         0                            0       0         0   \n",
       "2                         0                            0       0         0   \n",
       "3                         0                            0       0         0   \n",
       "4                         0                            0       0         0   \n",
       "5                         0                            0       0         0   \n",
       "6                         0                            0       0         0   \n",
       "7                         0                            0       0         0   \n",
       "8                         0                            0       0         0   \n",
       "9                         0                            0       0         0   \n",
       "\n",
       "   Pulmonary  ...  Obesity  Depression  Hypertension  Drugs  Alcohol  \\\n",
       "0          0  ...        0           0             0      0        0   \n",
       "1          0  ...        0           0             1      0        0   \n",
       "2          0  ...        0           0             1      0        0   \n",
       "3          1  ...        0           0             0      0        0   \n",
       "4          0  ...        0           0             1      0        0   \n",
       "5          0  ...        0           0             0      0        0   \n",
       "6          0  ...        0           0             0      0        0   \n",
       "7          0  ...        0           0             1      0        0   \n",
       "8          0  ...        0           1             0      0        0   \n",
       "9          0  ...        0           0             0      0        0   \n",
       "\n",
       "   First_Appointment_Date  Last_Appointment_Date  DateOfDeath  mortality  \\\n",
       "0              2013-04-27             2018-06-01          NaN          0   \n",
       "1              2005-11-30             2008-11-02   2008-11-02          1   \n",
       "2              2011-11-05             2015-11-13          NaN          0   \n",
       "3              2010-03-01             2016-01-17   2016-01-17          1   \n",
       "4              2006-09-22             2018-06-01          NaN          0   \n",
       "5              2006-10-22             2011-01-13          NaN          0   \n",
       "6              2015-01-20             2018-06-01          NaN          0   \n",
       "7              2013-03-25             2018-06-01          NaN          0   \n",
       "8              2008-08-04             2010-05-23          NaN          0   \n",
       "9              2014-07-01             2015-10-19          NaN          0   \n",
       "\n",
       "   Age_years  \n",
       "0  52.843258  \n",
       "1  55.373032  \n",
       "2  68.876112  \n",
       "3  35.433265  \n",
       "4  31.865845  \n",
       "5  27.126626  \n",
       "6  56.971937  \n",
       "7  62.168378  \n",
       "8  63.238877  \n",
       "9  60.210815  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = \\\n",
    "    pd.read_csv(r'PatientAnalyticFile.csv',\n",
    "                nrows=10)\n",
    "# Create mortality variable\n",
    "df2['mortality'] = \\\n",
    "    np.where(df2['DateOfDeath'].isnull(),\n",
    "             0,1)\n",
    "# Convert dateofBirth to date\n",
    "df2['DateOfBirth'] = \\\n",
    "    pd.to_datetime(df2['DateOfBirth'])\n",
    "# Calculate age in years as of 2015-01-01\n",
    "df2['Age_years'] = \\\n",
    "    ((pd.to_datetime('2015-01-01') - df2['DateOfBirth']).dt.days/365.25)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.308399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.447822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.192013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.651113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.847723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.108915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.535942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.822332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.881330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.714445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age_years\n",
       "0   0.308399\n",
       "1   0.447822\n",
       "2   1.192013\n",
       "3  -0.651113\n",
       "4  -0.847723\n",
       "5  -1.108915\n",
       "6   0.535942\n",
       "7   0.822332\n",
       "8   0.881330\n",
       "9   0.714445"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## standardized numeric cols\n",
    "X_num = (df2.loc[:,num_cols] - mean_all)/stdev_all\n",
    "X_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mortality ~ Gender + Race + Myocardial_infarction + Congestive_heart_failure + Peripheral_vascular_disease + Stroke + Dementia + Pulmonary + Rheumatic + Peptic_ulcer_disease + LiverMild + Diabetes_without_complications + Diabetes_with_complications + Paralysis + Renal + Cancer + LiverSevere + Metastatic_solid_tumour + HIV + Obesity + Depression + Hypertension + Drugs + Alcohol + mortality'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get dummy coded categoricals\n",
    "for col in cat_cols:\n",
    "    df2[col] = pd.Categorical(df2[col],\n",
    "                              categories=dict_unique[col])\n",
    "## use patsy to create formula of these\n",
    "formula = \"mortality ~ \" + \" + \".join(cat_cols)\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use Patsy to create model matrices\n",
    "Y,X_cat = dmatrices(formula,\n",
    "                    df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## join in the continuous X\n",
    "X_all = np.concatenate([X_cat,X_num],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:,1]\n",
    "classes_potential = np.array([0,1])\n",
    "classes_potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.09, NNZs: 1, Bias: 0.000000, T: 500, Avg. loss: 0.689114\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.09, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.683806\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.09, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.679597\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.09, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.677449\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.09, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.677750\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.09, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.675638\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.677372\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.677242\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.674696\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.675598\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.675079\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.676038\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.674875\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.674419\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.678431\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.08, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.675907\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.678451\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.677391\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.673917\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.675070\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.676041\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.677758\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.675875\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.677626\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.675913\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.674610\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.676823\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 2, Bias: 0.000000, T: 500, Avg. loss: 0.677144\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.678849\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.677531\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.676035\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 2, Bias: 0.000000, T: 500, Avg. loss: 0.674470\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 2, Bias: 0.000000, T: 500, Avg. loss: 0.677894\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 2, Bias: 0.000000, T: 500, Avg. loss: 0.676767\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 2, Bias: 0.000000, T: 500, Avg. loss: 0.676555\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 2, Bias: 0.000000, T: 500, Avg. loss: 0.676891\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 2, Bias: 0.000000, T: 500, Avg. loss: 0.677159\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.678523\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.679098\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 500, Avg. loss: 0.674924\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1, fit_intercept=False, loss='log', penalty='elasticnet',\n",
       "              tol=1e-06, verbose=1, warm_start=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## combine into one big step!!\n",
    "## setup the partial fit operator\n",
    "classes_potential = np.array([0,1])\n",
    "\n",
    "## Set our model\n",
    "clf = linear_model.SGDClassifier(fit_intercept=False, # already have the intercept\n",
    "                                 loss='log',\n",
    "                                 penalty='elasticnet',\n",
    "                                 warm_start=True,\n",
    "                                 alpha=1,\n",
    "                                 verbose=1,\n",
    "                                 tol=1e-6)\n",
    "\n",
    "## establish generator to yield data\n",
    "df_patient_gen3 = \\\n",
    " pd.read_csv(r'PatientAnalyticFile.csv',\n",
    "             chunksize=500)\n",
    "## test the loop!\n",
    "for n,chunk in enumerate(df_patient_gen3):\n",
    "    ## process data first\n",
    "    # Create mortality variable\n",
    "    chunk['mortality'] = \\\n",
    "        np.where(chunk['DateOfDeath'].isnull(),\n",
    "                 0,1)\n",
    "    # Convert dateofBirth to date\n",
    "    chunk['DateOfBirth'] = \\\n",
    "        pd.to_datetime(chunk['DateOfBirth'])\n",
    "    # Calculate age in years as of 2015-01-01\n",
    "    chunk['Age_years'] = \\\n",
    "        ((pd.to_datetime('2015-01-01') - chunk['DateOfBirth']).dt.days/365.25)\n",
    "    # standardized numeric cols\n",
    "    X_num = (chunk.loc[:,num_cols] - mean_all)/stdev_all\n",
    "    ## get dummy coded categoricals\n",
    "    for col in cat_cols:\n",
    "        chunk[col] = pd.Categorical(chunk[col],\n",
    "                                    categories=dict_unique[col])\n",
    "    ## use Patsy to create model matrices\n",
    "    Y,X_cat = dmatrices(formula,\n",
    "                        chunk)\n",
    "    ## join in the continuous X\n",
    "    X_all = np.concatenate([X_cat,X_num],axis=1)\n",
    "    \n",
    "    ## update the fit, one full EPOCH?\n",
    "    clf.partial_fit(X_all,Y[:,1],classes=classes_potential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
